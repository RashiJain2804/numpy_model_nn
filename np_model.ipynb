{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARING THE DATASET\n",
    "inp = np.random.randn(20, 32, 32,1)  #Dataset having 20 examples\n",
    "Y=np.random.randint(9,size=20)   #True Labels\n",
    "b = np.zeros((20,10))            #One-hot Encoding of True Labels\n",
    "b[np.arange(20), Y] = 1\n",
    "Y=b                              #One-hot encoded labels\n",
    "\n",
    "W1 = np.random.randn(3, 3,1, 4)\n",
    "W2 =np.random.randn(1024,10)\n",
    "b1 = np.random.randn(1, 1,1, 4)\n",
    "b2=np.random.randn(1,10)\n",
    "\n",
    "stride = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single(a_slice, W, b):\n",
    "    # Element-wise product between a_slice and W. Add bias.\n",
    "    s = np.multiply(a_slice, W) + b\n",
    "    # Sum over all entries of the volume s\n",
    "    Z = np.sum(s)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(inp, W, b, stride, m):\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = inp.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    pad = 0\n",
    "    # Computing the dimensions of the CONV output volume using the formula given.\n",
    "    n_H = math.ceil((n_H_prev - f + (2 * pad)) / stride) + 1\n",
    "    n_W = math.ceil((n_W_prev - f + (2 * pad)) / stride) + 1\n",
    "    \n",
    "    \n",
    "    # Initialize the output volume Z with zeros.\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    \n",
    "    for i in range(m):                                 # loop over the batch of training examples\n",
    "        inp_prev= inp[i]                               # Select ith training example\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                                                       # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    \n",
    "                    if (w == (n_W - 1) and h!= (n_H-1)):\n",
    "                        inp_slice = inp_prev[vert_start:vert_end, horiz_start: horiz_end - 1, :]\n",
    "                        tmp = W[:,0:2,:]\n",
    "                        tmp = tmp[...,c]\n",
    "                        Z[i, h, w, c] = conv_single(inp_slice, tmp, b[...,c])\n",
    "                    \n",
    "                    elif ( h == ((n_H - 1))):\n",
    "                        if (w == (n_W - 1)):\n",
    "                            inp_slice = inp_prev[vert_start:vert_end-1, horiz_start: horiz_end-1, :]\n",
    "                            tmp = W[0:2,0:2,:]\n",
    "                            tmp = tmp[..., c]\n",
    "                            Z[i, h, w, c] = conv_single(inp_slice, tmp, b[...,c])\n",
    "                        else:\n",
    "                            inp_slice = inp_prev[vert_start:vert_end-1, horiz_start: horiz_end, :]\n",
    "                            tmp = W[0:2,:,:]\n",
    "                            tmp = tmp[...,c]\n",
    "                            Z[i, h, w, c] = conv_single(inp_slice, tmp, b[...,c])\n",
    "                    \n",
    "                    else:\n",
    "                        inp_slice = inp_prev[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                        Z[i, h, w, c] = conv_single(inp_slice, W[...,c], b[...,c])\n",
    "    \n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (inp, W, b, stride)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A =  np.maximum(Z, 0)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_forward(A, W2, b2):\n",
    "    Z = np.dot(A,W2) + b2\n",
    "    cache = (A, W2, b2)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_model(inp, W1, W2,b1, b2, stride):\n",
    "    caches = []\n",
    "    Z, cache = conv_forward(inp, W1, b1, stride,20)\n",
    "    caches.append(tuple((cache)))\n",
    "    Z = Z.reshape(20, -1)\n",
    "    A, cache = relu(Z)\n",
    "    caches.append(cache)\n",
    "    A, cache = fc_forward(A, W2, b2)\n",
    "    caches.append(tuple((cache)))\n",
    "    AL=softmax(A)\n",
    "    output=np.argmax(AL,axis=1)\n",
    "    \n",
    "    return AL,caches,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the forward model.\n",
    "AL,caches,output=forward_model(inp,W1,W2,b1,b2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[0]\n",
    "    dW = np.dot(cache[0].T, dZ) / m\n",
    "    db = np.squeeze(np.sum(dZ, axis=0, keepdims=True)) / m\n",
    "    db.reshape(1,10)\n",
    "    dA_prev = np.dot(dZ, cache[1].T)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(AL, dAL):\n",
    "    dZ = np.zeros((20,10))                #initializing dZ to correct shape\n",
    "    for i in range(10):\n",
    "        a = AL[:, i]\n",
    "        dz = np.exp(a)*(np.sum(np.exp(a), axis = 0) - np.exp(a[0]))/(np.square(np.sum(np.exp(a), axis = 0)))  #calculating derivative of softmax\n",
    "        dZ[:, i] = dz\n",
    "    dZ = np.multiply(dZ, dAL)            #dZ = dA * g'(z)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "   \n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, dZ should be also zero\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, s) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        a_prev = A_prev[i]                   # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        da_prev = dA_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_model(AL, Y, cache):\n",
    "    grads = {}\n",
    "    m = AL.shape[0]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    dAL = dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    current_cache = cache[-1]\n",
    "    grads[\"dA2\"], grads[\"dW2\"], grads[\"db2\"] = linear_backward(softmax_backward(AL,dAL), current_cache)\n",
    "    dZ2=relu_backward(grads[\"dA2\"],cache[-2])\n",
    "    dZ2=dZ2.reshape((20,16,16,4))\n",
    "    grads[\"dA1\"],grads[\"dW1\"],grads[\"db1\"]=conv_backward(dZ2,cache[-3])\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1,b1,W2,b2, grads, lr=0.1):\n",
    "    W1=W1-lr*grads[\"dW1\"]\n",
    "    b1=b1-lr*grads[\"db1\"]\n",
    "    W2=W2-lr*grads[\"dW2\"]\n",
    "    b2=b2-lr*grads[\"db2\"]\n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamrashi28/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in less_equal\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    AL,caches,output=forward_model(inp,W1,W2,b1,b2, 2)\n",
    "    grads=backward_model(AL, Y, caches)\n",
    "    W1,b1,W2,b2=update_parameters(W1,b1,W2,b2, grads, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
